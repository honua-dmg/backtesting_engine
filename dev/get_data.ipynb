{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from tkinter import Tk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pydrive.drive import GoogleDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "import os\n",
    "failed = []\n",
    "\n",
    "\n",
    "def get_scanclause(url:str)->str:\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        url: url of link\n",
    "    Returns:\n",
    "        string containing scan clause\n",
    "    Description:\n",
    "        uses selenium and tkinter to scrape the scan clause from the link\n",
    "\n",
    "\n",
    "        steps:\n",
    "        1. initialise a driver\n",
    "        2. get url via driver\n",
    "        3. find the copy button (next to the 'stock passes ... in ... segment')\n",
    "        4. click the button to copy scan clause to clipboard\n",
    "        5. access clipboard via tkinter\n",
    "        6. copy clause\n",
    "        7. destroy tkinter screen to avoid build up\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # 1\n",
    "    driver = webdriver.Edge()              \n",
    "    # 2        \n",
    "    driver.get(url)              \n",
    "    #3                \n",
    "    element = driver.find_element(By.XPATH, '/html/body/div[2]/div[2]/div[2]/div/div/div/div[2]/div/div[2]/div[1]/div/div/i')\n",
    "    #4\n",
    "    element.click()\n",
    "    #5\n",
    "    a = Tk()\n",
    "    #6\n",
    "    clipboard = a.clipboard_get()\n",
    "    #7\n",
    "    a.destroy()\n",
    "    return clipboard\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clause_parser(clause):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        scan_clause : the scan clause of the given link\n",
    "                    (contains data about the conditions imposed for the backtest process)\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        final_clause: updated scan clause with only day parameters\n",
    "   \n",
    "   steps:\n",
    "        1. split clause by spaces\n",
    "        2. check for kewords in frames or framesly\n",
    "            a. if in framesly, replace with Daily\n",
    "            b. if in frames, replace with day or days depending on preceeding number (if number = 1, `day`, or else `days`)\n",
    "        3. return joined clause\n",
    "    \"\"\"\n",
    "    #1\n",
    "    split_clause = clause.split(' ')\n",
    "    frames = ['minute','hour','week','weeks','month','months','quarter','year','years']\n",
    "    framesly = ['Weekly','Monthly','Quaterly','Yearly']\n",
    "    for i in range(len(split_clause)):\n",
    "        #2a\n",
    "        if split_clause[i] in framesly:\n",
    "            split_clause[i] = 'Daily'\n",
    "        #2b\n",
    "        elif split_clause[i] in frames:\n",
    "            if split_clause[i-1] == '1':\n",
    "                split_clause[i] = 'day'\n",
    "            else:\n",
    "                split_clause[i] = 'days'\n",
    "    #3\n",
    "    return ' '.join(split_clause)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def results(url:str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        url: the url of the backtest link\n",
    "        scan_clause: the scan clauseo f the given data link\n",
    "        path: a path to save the dataframe in csv format to\n",
    "       \n",
    "    Returns:\n",
    "        csv file of backtest results\n",
    "\n",
    "\n",
    "    Description:\n",
    "        sends a post request with the scan clause to recieve the backtest results\n",
    "        parses json results received into a pandas dataframe\n",
    "\n",
    "\n",
    "        steps:\n",
    "       \n",
    "        1. open a requests session\n",
    "        2. get csrf token via a get request to the url\n",
    "        3. send a post request with the csrf token and scan clause as payload to get data\n",
    "        4. parsing the data\n",
    "        5. if a path parameter is specified, the parsed data (in a dataframe), is converted to a csv file and stored at the specified path.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    scan_clause = clause_parser(get_scanclause(url))\n",
    "    # 1\n",
    "    with requests.session() as s:\n",
    "        condition = {\"scan_clause\": scan_clause}\n",
    "        #2\n",
    "        r_data = s.get(url)\n",
    "        soup = bs(r_data.content,\"lxml\")                            # converting source code into a soup object\n",
    "        meta = soup.find(\"meta\",{\"name\":\"csrf-token\"})['content']   # serching for the csrf-token meta tag; specifically getting it's content\n",
    "        print(meta)\n",
    "        header = {'x-csrf-token':meta}\n",
    "        # 3\n",
    "        data = s.post(\"https://chartink.com/backtest/process\",headers=header,data=condition).json()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # parsing the data into a prettier format:\n",
    "    #4\n",
    "    date = data['metaData'][0]['tradeTimes']\n",
    "    stonks = data['aggregatedStockList']\n",
    "    finale = []\n",
    "    stonks_in_date = []\n",
    "\n",
    "\n",
    "    for j in range(0,len(stonks[0]),3):\n",
    "        stonks_in_date.append(stonks[0][j])\n",
    "    finale.append({\"date\":date[0] ,'stocks' :stonks_in_date})\n",
    "    for i in range(1,len(date)-1):\n",
    "        stonks_in_date = []\n",
    "        for j in range(0,len(stonks[i]),3):\n",
    "            stonks_in_date.append(stonks[i][j])\n",
    "        finale.append({\"date\":date[i] ,'stocks' :stonks_in_date})\n",
    "    return pd.DataFrame(finale)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_files(urls,start=0,stop=None):\n",
    "    if stop == None:\n",
    "        stop = len(urls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with open('../data/failed.csv','a+') as failed:\n",
    "        if os.path.getsize('../data/failed.csv') ==0:\n",
    "            failed.write('url,failure\\n')\n",
    "\n",
    "\n",
    "    with open('../data/masterfile.csv', 'a+') as f:\n",
    "        if os.path.getsize('../data/masterfile.csv') ==0:\n",
    "            f.write('url,loc\\n')\n",
    "\n",
    "\n",
    "        try:\n",
    "            existing_urls = pd.read_csv('../data/masterfile.csv')['url'].to_list()\n",
    "            print(len(existing_urls))\n",
    "        except Exception:\n",
    "            pass\n",
    "        for i in range(len(urls)):\n",
    "            file_name = urls[i].split('/')[-1]\n",
    "            if urls[i] in existing_urls:\n",
    "                continue\n",
    "            if 'fundamental' in ' '.join(file_name.split('-')):\n",
    "                print(f'{file_name} with the url: {urls[i]} failed due to it being a fundamental test')\n",
    "                with open('./data/failed.csv','a+') as failed:\n",
    "                    failed.write(f'{urls[i]},fundamental\\n')\n",
    "                print('*'*80)                    \n",
    "            try:\n",
    "                data = results(urls[i])\n",
    "                file_path = '../data/'+file_name +'.csv'\n",
    "                f.write(f'{urls[i]},{file_name}.csv\\n')\n",
    "                data.to_csv(file_path)\n",
    "                print(f'{file_name} saved successfully')\n",
    "                print('*'*80)\n",
    "            except Exception as e:\n",
    "                print(f'{file_name} with the url: {urls[i]} failed due to {e}')\n",
    "                with open('../data/failed.csv','a+') as failed:\n",
    "                    failed.write(f'{urls[i]},{e}\\n')\n",
    "                print('*'*80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
